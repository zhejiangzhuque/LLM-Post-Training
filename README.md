# LLM-Post-Training

[PPT](./面向强推理LLM的强化学习进阶.pdf)

DeepSeek-R1的发布揭示了工程进步及强化学习在大模型训练领域的强大潜力，激发了学界对强化学习（RL）提升大型语言模型（LLM）推理能力的广泛探索。后续研究揭示，小规模、高质量数据的监督微调（SFT）即可取得显著效果，且对特定模型，RL 在增强推理能力上可能比SFT更具优势。在此基础上，通过引入解耦裁剪、动态采样、词元级损失及软性长度惩罚等机制改良GRPO算法，并结合课程学习策略，显著提升模型在复杂推理任务上的表现与训练稳定性，使得中等规模模型性能得以大幅跃升。

为验证方法的泛化性，团队在中文数学及医学等多个数据集上进行了跨语言、跨专业领域的实验，结果表明优化后的RL训练框架能够有效迁移，显著提升了模型在这些特定领域的推理能力，甚至使小模型在专有任务上超越了大型通用模型。团队进一步探讨了KL散度惩罚系数等关键超参数对训练动态和最终性能的影响，并展望了未来在深化RL理解（如反思机制）、探索可扩展RL新范式（MCTS、DAPO）、克服硬件（内存/通信）瓶颈以及应对高质量数据需求等方面的挑战与方向。